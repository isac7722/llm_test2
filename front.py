# API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import ChatMessage
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from utils import print_messages, StreamHandler
import streamlit as st
from langchain_core.prompts import PromptTemplate
from main import ensemble_retriever

from dotenv import load_dotenv
# API í‚¤ ì •ë³´ ë¡œë“œ
load_dotenv()


st.set_page_config(page_title="LangChain Master", page_icon="ğŸ‘¨ğŸ»â€ğŸ’»")
st.title("LangChain Master ğŸ‘¨ğŸ»â€ğŸ’»")

if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ì±„íŒ… ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” store ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜
if "store" not in st.session_state:
    st.session_state["store"] = dict()

with st.sidebar:
    session_id = st.text_input("Session ID", value="root")

    clear_btn = st.button("ëŒ€í™”ê¸°ë¡ ì´ˆê¸°í™”")
    if clear_btn:
        st.session_state["messages"] = []
        st.session_state["store"] = dict()
        st.rerun()

# ì´ì „ ëŒ€í™”ê¸°ë¡ì„ ì¶œë ¥í•´ì£¼ëŠ” ì½”ë“œ
print_messages()


def get_session_history(session_ids: str) -> BaseChatMessageHistory:
    if session_ids not in st.session_state["store"]:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ì¥
        st.session_state["store"][session_ids] = ChatMessageHistory()
    return st.session_state["store"][session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜


if user_input := st.chat_input("ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”"):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
    st.chat_message("user").write(f"{user_input}")
    st.session_state["messages"].append(
        ChatMessage(role="user", content=user_input))

    # LLMì„ ì‚¬ìš©í•˜ì—¬ AIì˜ ë‹µë³€ì„ ìƒì„±

    # AIì˜ ë‹µë³€
    with st.chat_message("assistant"):
        # stream_handler = StreamHandler(st.empty())

        # # 1. ëª¨ë¸ ìƒì„±
        # llm = ChatOpenAI(streaming=True, callbacks=[stream_handler])




        ## LLM ì •ì˜
        from langchain.callbacks.base import BaseCallbackHandler
        from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
        from langchain_core.callbacks.manager import CallbackManager
        from langchain_core.runnables import ConfigurableField
        from langchain_core.output_parsers import StrOutputParser
        from langchain_core.runnables import RunnablePassthrough

        from langchain_community.chat_models import ChatOllama
        from langchain_openai import ChatOpenAI
        from langchain_anthropic import ChatAnthropic

        class StreamCallback(BaseCallbackHandler):
            def on_llm_new_token(self, token:str, **kwargs):
                print(token, end="", flush=True)

        llm = ChatOpenAI(
            model="gpt-4-turbo-preview",
            temperature=0,
            streaming=True,
            callbacks=[StreamCallback()],
        ).configurable_alternatives(
            # ì´ í•„ë“œì— idë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤
            # ìµœì¢… ì‹¤í–‰ ê°€ëŠ¥í•œ ê°ì²´ë¥¼ êµ¬ì„±í•  ë•Œ, ì´ idë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ í•„ë“œë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            ConfigurableField(id="llm"),
            # ê¸°ë³¸ í‚¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
            default_key="gpt4",
            claude=ChatAnthropic(
                model="claude-3-opus-20240229",
                temperature=0,
                streaming=True,
                callbacks=[StreamCallback()],
            ),
            gpt3=ChatOpenAI(
                model="gpt-3.5-turbo",
                temperature=0,
                streaming=True,
                callbacks=[StreamCallback()],
            ),
            ollama=ChatOllama(
                model="EEVE-Korean-10.8B:long",
                callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
            ),
        )





        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        prompt = PromptTemplate.from_template(
            """
                ë‹¹ì‹ ì€ 20ë…„ì°¨ AI ê°œë°œìì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–¸ì§€ ì§ˆë¬¸ì— ëŒ€í•˜ì—¬ ìµœëŒ€í•œ ë¬¸ì„œì˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
                ë¬¸ì„œëŠ” Python ì½”ë“œì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ë‹µë³€ì„ ì‘ì„±í•  ë•Œì—ëŠ” Python ì½”ë“œì— ëŒ€í•œ ìƒì„¸í•œ code snippetì„ í¬í•¨í•˜ì—¬ ì‘ì„±í•´ì£¼ì„¸ìš”.
                ìµœëŒ€í•œ ìì„¸í•˜ê²Œ ë‹µë³€í•˜ê³ , í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°, "ë¬¸ì„œì— ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•´ ì£¼ì„¸ìš”.
                ë‹µë³€ì€ ì¶œì²˜(source)ë¥¼ ë°˜ë“œì‹œ í‘œê¸°í•´ ì£¼ì„¸ìš”.

                #ì°¸ê³ ë¬¸ì„œ:
                {context}

                #ì§ˆë¬¸:
                {question}

                #ë‹µë³€:

                ì¶œì²˜:
                - source1
                - source2
                - ...
            """
        )

        # chain = prompt | llm


        rag_chain = (
            {"context": ensemble_retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )







        # chain_with_memory = (
        #     RunnableWithMessageHistory(  # RunnableWithMessageHistory ê°ì²´ ìƒì„±
        #         rag_chain,  # ì‹¤í–‰í•  Runnable ê°ì²´
        #         get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
        #         input_messages_key="question",  # ì‚¬ìš©ì ì§ˆë¬¸ì˜ í‚¤
        #         history_messages_key="history",  # ê¸°ë¡ ë©”ì„¸ì§€ì˜ í‚¤
        #     )
        # )

        # response = chain.invoke({"question":user_input})
        response = rag_chain.invoke(

            # {"question": user_input},
            {user_input},
            # # session ID ì„¤ì •
            # config={"configurable": {"session_id": session_id}},
        )

        msg = response.content

        # st.write(msg)
        st.session_state["messages"].append(
            ChatMessage(role="assistant", content=msg))
